#sim added
#optimal selection of component numbers added

#' Simulating data from finite mixture of regression models
#'
#' This function allows you to simulate responses of finite mixture of linear regression models
#' @param x annotation matrix (no of SNPs * no of annotations)
#' @param ssd component standard deviations
#' @param beta regression coefficient  matrix ((1 + no of annotations)  * no of components)
#' @param prob component probabilities
#' @return a response vector generated by finite mixture of regression modelling#' 
#' @keywords arog data simulation
#' @author Sunyoung Shin\email{shin@@stat.wisc.edu}
#' @examples
#' \dontrun{
#' n <- 100
#' p <- 50
#' x <- matrix(rnorm(n*p),n,p)
#' beta <- cbind(c(0,rep(3,5),rep(0,p-5)),c(0,rep(-1,5),rep(0,p-5)))
#' prob <- c(0.5,0.5)
#' ssd <- c(0.5,0.5)
#' y <- sim(x=x,ssd=ssd,beta=beta,prob=prob)
#' }
#' @export
sim <- function(x,prob,beta,ssd){
    pred.x <- cbind(rep(1,n),x)
    k <- length(prob)
    n <- dim(pred.x)[1]
    y <- numeric(n)
    s <- sample(1:k,n,replace=TRUE,p=prob)
    for (i in 1:n){
        y[i] <- rnorm(1,mean=beta[,s[i]]%*%pred.x[i,],sd=ssd[s[i]])
    }
    y
}

fmrlasso.rlasso.try<-function(cluster, sel.inds, cls, y, x) {
if(length(cls)==1) {
  subn<-list(seq(length(cluster)))
  subp<-list(setdiff(sel.inds[which(sel.inds[,2]==cls), 1], 1)-1)
} else {
  subn<-mapply(function(x) which(x==cluster), cls, SIMPLIFY=FALSE)
  subp<-mapply(function(x) setdiff(sel.inds[which(sel.inds[,2]==x), 1], 1)-1, cls, SIMPLIFY=FALSE)
}
  subp.len<-mapply(length, subp, SIMPLIFY=TRUE); subn.len<-mapply(length, subn, SIMPLIFY=TRUE)
  ind.0<-which(subp.len==0)
  cls.0<-cls[ind.0]
  subtryintercept<-mapply(function(subn.c, subp.c) {sub.y<-y[subn.c];is(try(lm(sub.y~1), silent=TRUE), "try-error")}, subn[ind.0], subp[ind.0], SIMPLIFY=TRUE)
  ind.int2.0<-ind.0[which(subtryintercept==FALSE)]
  cls.int2.0<-cls[ind.int2.0]

  ind.1<-sort(intersect(which(subp.len==1), which(subn.len>2)))
  cls.1<-cls[ind.1]
  subtryuni<-mapply(function(subn.c, subp.c) {sub.y<-y[subn.c]; sub.x<-x[subn.c,subp.c]; is(try(lm(sub.y~sub.x), silent=TRUE), "try-error")}, subn[ind.1], subp[ind.1], SIMPLIFY=TRUE)
  ind.int2.1<-ind.1[which(subtryuni==FALSE)]
  cls.int2.1<-cls[ind.int2.1]

  ind.int<-which(mapply(length, subp, SIMPLIFY=TRUE)>1)
  cls.int<-cls[ind.int]
  subtrylasso<-mapply(function(subn.c, subp.c) {sub.y<-y[subn.c]; sub.x<-x[subn.c, subp.c]; is(try(glmnet(sub.x, sub.y, standardize=FALSE), silent=TRUE), "try-error")}, subn[ind.int], subp[ind.int], SIMPLIFY=TRUE)
  ind.int2<-ind.int[which(subtrylasso==FALSE)]
  cls.int2<-cls[ind.int2]

  results<-list(subn=subn, subp=subp, ind=list(ind.int2.0, ind.int2.1, ind.int2), cls=list(cls.int2.0, cls.int2.1, cls.int2))
  results
}

fmrlasso.rlasso.fit.bic<-function(subn, subp, ind.int2, y, x) {
  sublasso<-mapply(function(subn.c, subp.c) {sub.y<-y[subn.c]; sub.x<-x[subn.c, subp.c]; glmnet(sub.x, sub.y, standardize=FALSE)}, subn[ind.int2], subp[ind.int2], SIMPLIFY=FALSE)
  sublasso.ind<-mapply(function(elnet, subn.c, subp.c) {sub.y<-y[subn.c]; sub.x<-x[subn.c, subp.c]; fullbeta<-rbind(elnet$a0, as.matrix(elnet$beta)); subinds0<-unique(alply(fullbeta, 2, function(x) which(x!=0))); subinds<-subinds0[which(mapply(function(ids) length(ids)<length(subn.c), subinds0))]; subinds}, sublasso, subn[ind.int2], subp[ind.int2], SIMPLIFY=FALSE)
    sublasso.lm<-mapply(function(elnet, subinds, subn.c, subp.c) {sub.y<-y[subn.c]; sub.x<-x[subn.c, subp.c]; rlasso=lapply(subinds, function(ids) if(length(ids)>1) {lm(sub.y~sub.x[,setdiff(ids, 1)-1])} else {lm(sub.y~1)});  rlasso}, sublasso, sublasso.ind, subn[ind.int2], subp[ind.int2], SIMPLIFY=FALSE)
  sublasso.bic=mapply(function(lms) mapply(BIC, lms), sublasso.lm, SIMPLIFY=FALSE)
  sublasso.min.ind<-mapply(which.min, sublasso.bic, SIMPLIFY=TRUE)
  sublasso.opt<-mapply(function(lms, subinds, min.ind, subp.c) {coef=rep(0, length(subp.c)+1); coef[subinds[[min.ind]]]<-coef(lms[[min.ind]]); list(coef=coef, ssd=summary(lms[[min.ind]])$sigma)}, sublasso.lm, sublasso.ind, sublasso.min.ind, subp[ind.int2], SIMPLIFY=FALSE)
  sublasso.opt
}


fmrlasso.rlasso<-function(fmrlasso.coef, fmrlasso.ssd, fmrlasso.cluster,  y, x) {
fmrlasso.sel.inds<-which(fmrlasso.coef!=0, arr.ind=TRUE)
fmrlasso.cls<-as.numeric(names(which(table(fmrlasso.cluster)>1)))
fmrlasso.rlasso.coef<-fmrlasso.coef
fmrlasso.rlasso.ssd<-fmrlasso.ssd
#if(all(fmrlasso.coef[-1, fmrlasso.cls]==0)==FALSE) {

  fmrlasso.rlasso.trial<-fmrlasso.rlasso.try(fmrlasso.cluster, fmrlasso.sel.inds, fmrlasso.cls, y, x)
  if(length(fmrlasso.rlasso.trial$ind[[1]])>0) {
    intercept.fit<-mapply(function(subn.c) {sub.y<-y[subn.c]; list(coef=coef(lm(sub.y~1)), ssd=summary(lm(sub.y~1))$sigma)}, fmrlasso.rlasso.trial$subn[fmrlasso.rlasso.trial$ind[[1]]], SIMPLIFY=FALSE)
  for(i in seq(length(fmrlasso.rlasso.trial$ind[[1]]))) {
  fmrlasso.rlasso.coef[1, fmrlasso.rlasso.trial$cls[[1]][i]]<-intercept.fit[[i]]$coef
  fmrlasso.rlasso.ssd[fmrlasso.rlasso.trial$cls[[1]][i]]<-intercept.fit[[i]]$ssd
}
}

  if(length(fmrlasso.rlasso.trial$ind[[2]])>0) {
    uni.fit<-mapply(function(subn.c, subp.c) {sub.y<-y[subn.c]; sub.x<-x[subn.c, subp.c]; list(coef=coef(lm(sub.y~sub.x)), ssd=summary(lm(sub.y~sub.x))$sigma)}, fmrlasso.rlasso.trial$subn[fmrlasso.rlasso.trial$ind[[2]]], fmrlasso.rlasso.trial$subp[fmrlasso.rlasso.trial$ind[[2]]], SIMPLIFY=FALSE)
  for(i in seq(length(fmrlasso.rlasso.trial$ind[[2]]))) {
  add.inds.i<-fmrlasso.sel.inds[fmrlasso.sel.inds[,2]==fmrlasso.rlasso.trial$cls[[2]][i],1]
  fmrlasso.rlasso.coef[add.inds.i, fmrlasso.rlasso.trial$cls[[2]][i]]<-uni.fit[[i]]$coef
  fmrlasso.rlasso.ssd[fmrlasso.rlasso.trial$cls[[2]][i]]<-uni.fit[[i]]$ssd
}
}

  if(length(fmrlasso.rlasso.trial$ind[[3]])>0) {
    fmrlasso.rlasso.fit<-fmrlasso.rlasso.fit.bic(fmrlasso.rlasso.trial$subn, fmrlasso.rlasso.trial$subp, fmrlasso.rlasso.trial$ind[[3]], y, x)
  for(i in seq(length(fmrlasso.rlasso.trial$ind[[3]]))) {
  add.inds.i<-fmrlasso.sel.inds[fmrlasso.sel.inds[,2]==fmrlasso.rlasso.trial$cls[[3]][i],1]
  fmrlasso.rlasso.coef[add.inds.i, fmrlasso.rlasso.trial$cls[[3]][i]]<-fmrlasso.rlasso.fit[[i]]$coef
  fmrlasso.rlasso.ssd[fmrlasso.rlasso.trial$cls[[3]][i]]<-fmrlasso.rlasso.fit[[i]]$ssd
}
}
#}
list(coef=fmrlasso.rlasso.coef, ssd=fmrlasso.rlasso.ssd)
}

ini.ex.k<-function(hclust.tot, k, n) {
  if(k>1) {
  memb.tot<-cutree(hclust.tot, k = k)
  t(apply(as.matrix(memb.tot), 1, function(i) {bin=rep(1, k); bin[i]<-9; bin<-bin/(8+k); bin}))
  #t(apply(as.matrix(memb.tot), 1, function(i) {bin=rep(0, opt.k); bin[i]<-1; bin})
} else {
  matrix(1, n, 1)
}
}

#' ARoG function
#'
#' This function allows you to fit ARoG (I) and ARoG (II)
#' @param y GWAS association measure vector
#' @param x annotation matrix (no of SNPs * no of annotations)
#' @param lbd.min smallest tuning parameter
#' @param lbd.mult multiplication factor for largest tuning parameter Defaults to 1.
#' @param ln numbers of tuning parameters searched Defaults to 10.
#' @param ssd.ini initial values for residual standard deviations
#' @param k number of clusters
#' @param term termination criterion Defaults to 10^{-6}
#' @param maxiter maximal number of EM-iterations Defaults to 1000.
#' @return a list object containing the following components:
#' \tabular{ll}{
#' opt.ind.lbd.bic \tab the optimal tuning parameter index, value and corresponding BIC \cr
#' fmrlasso \tab a list of ARoG (I) results \cr
#' fmrlasso$k \tab no of components \cr
#' fmrlasso$prob \tab component probabilities \cr
#' fmrlasso$coef \tab ARoG (I) regression coefficient matrix \cr
#' fmrlasso$ssd \tab ARoG(I) component sample standard deviations \cr
#' fmrlasso$plik \tab value of penalized negative log-likelihood \cr
#' fmrlasso$bic \tab bic value \cr
#' fmrlasso$ex \tab posterior probabilities of SNPs to components (no of SNPs * no of components) \cr
#' fmrlasso$cluster \tab component assignments of SNPs \cr
#' fmrlasso$niter \tab no of EM algorithm iteratioins \cr
#' fmrlasso$warnings \tab check if there is any warning or not \cr
#' fmrlassopath \tab a list of object containing ARoG (I) results on a of tuning parameters. Subobjects for each tuning parameter value are created and they are the same as those of fmrlasso. \cr
#' fmrlasso.rlasso \tab a list of ARoG (II) results \cr
#' fmrlasso.rlasso$coef \tab ARoG (I) regression coefficient matrix \cr
#' fmrlasso.rlasso$ssd \tab ARoG(I) component sample standard deviations \cr
#' }
#' @keywords arog
#' @author Sunyoung Shin\email{shin@@stat.wisc.edu}
#' @examples
#' \dontrun{
#' n <- 100
#' p <- 50
#' x <- matrix(rnorm(n*p),n,p)
#' beta <- cbind(c(0,rep(3,5),rep(0,p-5)),c(0,rep(-1,5),rep(0,p-5)))
#' prob <- c(0.5,0.5)
#' ssd <- c(0.5,0.5)
#' y <- sim(x=x,ssd=ssd,beta=beta,prob=prob)
#' arogResults<-arog(y, x, lbd.min=1, lbd.mult=4, ln=10, k=2)
#' }
#' @export
arog<-function(y, x, lbd.min=0.1, lbd.mult=4, ln=4, k, ssd.ini=0.5, term=10^{-6}, maxiter=1000) {

#        if(all(is.numeric(y), is.numeric(x))==FALSE) {
#                  stop("Both x and y must be numeric")
#              } else if(
#              }
                      
pred.x=cbind(1, x)
n<-nrow(x); p<-ncol(x)

#FMRLASSO
lbd.max<-lambdamax(y, pred.x)          
la <- seq(lbd.min, lbd.mult*lbd.max,length=ln)
#perform fmrlasso path
tot0<-cbind(y, apply(x, 1, function(x) sqrt(mean(x^2))))
tot<-tot0%*%diag(1./apply(tot0, 2, sd))
dist.tot<-dist(tot, method="manhattan")
hclust.tot<-hclust(dist.tot)

ex.ini.k<-ini.ex.k(hclust.tot, k, n)
fmrlassopath.k<-fmrlassopath(pred.x, y, k, lambda=la, ssd.ini=ssd.ini, ex.ini=ex.ini.k, term=term, maxiter=maxiter)

bic.k<-fmrlassopath.k$bic
names(bic.k)<-la
opt.ind.k<-which.min(bic.k)
opt.bic.k<-bic.k[opt.ind.k]
opt.ind.lbd.bic.k<-c(opt.ind.k, la[opt.ind.k], opt.bic.k)
names(opt.ind.lbd.bic.k)<-c("ind", "lambda", "BIC")

#fit fmrlasso
fmrlasso.k<- fmrlasso(pred.x, y, k=k, lambda=opt.ind.lbd.bic.k[2], ssd.ini=0.5, ex.ini=ex.ini.k, term=term, maxiter=maxiter)

#FMRLasso-Refit Lasso
fmrlasso.rlasso.k<-fmrlasso.rlasso(fmrlasso.k$coef, fmrlasso.k$ssd, fmrlasso.k$cluster, y, x)

#Choice of no. of components

#FMRLasso, FMRLasso refit Lasso
results=list(opt.ind.lbd.bic=opt.ind.lbd.bic.k, fmrlasso=fmrlasso.k, fmrlassopath=fmrlassopath.k, fmrlasso.rlasso=fmrlasso.rlasso.k)

results
}

#' ARoG with optimal no of components function
#'
#' This function allows you to search optimal number of component functions among 1 to a specified maximum component number and fit ARoG (I) and ARoG (II) with the optimal number.
#' @param y GWAS association measure vector
#' @param x annotation matrix (no of SNPs * no of annotations)
#' @param lbd.min smallest tuning parameter
#' @param lbd.mult multiplication factor for largest tuning parameter Defaults to 1.
#' @param ln numbers of tuning parameters searched Defaults to 10.
#' @param ssd.ini initial values for residual standard deviations
#' @param maxK maximum candidate numbers of components
#' @param parallel parallel computing or not
#' @param nproc no of cores for parallelization
#' @param term termination criterion Defaults to 10^{-6}
#' @param maxiter maximal number of EM-iterations Defaults to 1000.
#' @return a list object containing the following components:
#' \tabular{ll}{
#' opt.ind.lbd.bic \tab the optimal tuning parameter index, value and corresponding BIC with the optimal no of components \cr
#' fmrlasso \tab a list of ARoG (I) results with the optimal no of components \cr
#' fmrlasso$k \tab optimal no of components \cr
#' fmrlasso$prob \tab component probabilities \cr
#' fmrlasso$coef \tab ARoG (I) regression coefficient matrix \cr
#' fmrlasso$ssd \tab ARoG(I) component sample standard deviations \cr
#' fmrlasso$plik \tab value of penalized negative log-likelihood \cr
#' fmrlasso$bic \tab bic value \cr
#' fmrlasso$ex \tab posterior probabilities of SNPs to components (no of SNPs * no of components) \cr
#' fmrlasso$cluster \tab component assignments of SNPs \cr
#' fmrlasso$niter \tab no of EM algorithm iteratioins \cr
#' fmrlasso$warnings \tab check if there is any warning or not \cr
#' fmrlassopath \tab a list of object containing ARoG (I) results on a of tuning parameters. Subobjects for each tuning parameter value are created and they are the same as those of fmrlasso. \cr
#' fmrlasso.rlasso \tab a list of ARoG (II) results  with the optimal no of components \cr
#' fmrlasso.rlasso$coef \tab ARoG (I) regression coefficient matrix \cr
#' fmrlasso.rlasso$ssd \tab ARoG(I) component sample standard deviations \cr
#' }
#' @keywords arog optimal number of components
#' @author Sunyoung Shin\email{shin@@stat.wisc.edu}
#' @examples
#' \dontrun{
#' n <- 100
#' p <- 50
#' x <- matrix(rnorm(n*p),n,p)
#' beta <- cbind(c(0,rep(3,5),rep(0,p-5)),c(0,rep(-1,5),rep(0,p-5)))
#' prob <- c(0.5,0.5)
#' ssd <- c(0.5,0.5)
#' y <- sim(x=x,ssd=ssd,beta=beta,prob=prob)
#' arogResults_comp<-arog_comp(y, x, maxK=4)
#' }
#' @import parallel
#' @export

arog_comp<-function(y, x, lbd.min=1, lbd.mult=4, ln=10, ssd.ini=0.5, maxK, parallel=TRUE, nproc=4, term=10^{-6}, maxiter=1000) {
    if(parallel==TRUE) {
         arog_comp<-mclapply(as.matrix(seq(maxK)), function(k) arog(y, x, lbd.min=lbd.min, lbd.mult=lbd.mult, ln=ln, k=k, ssd.ini=ssd.ini, term=term, maxiter=maxiter), mc.cores=nproc)
     } else {
         arog_comp<-lapply(as.matrix(seq(maxK)), function(k) arog(y, x, lbd.min=lbd.min, lbd.mult=lbd.mult, ln=ln, k=k, ssd.ini=ssd.ini, term=term, maxiter=maxiter))
     }
    
    bic_comp<-mapply(function(arog) arog$fmrlasso$bic, arog_comp)
    opt_comp<-which.min(bic_comp)
    arog_results<-arog_comp[[opt_comp]]
    arog_results
}
   
